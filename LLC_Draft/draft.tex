

\documentclass[12pt, titlepage, reqno]{article}
\usepackage{natbib} 
\usepackage{amsmath}

\title{Deep learning architectures in speech processing}

\begin{document}
\maketitle 


%\section{Outline}
%\begin{enumerate}
%	%From Dafydd's mail Aug 21, 2016 at 12:21 PM
%	\item A (very) brief overview of 'black box' and 'white/glass box' methods would be needed, starting with the old 'statistical vs. rule-based' fight. We can pool ideas about this
%	\item If possible we should have some kind of working demos to illustrate what we are concerned with.
%	%This being a review paper the editors would want an exhaustive exploration of the literature. While a working demo would have been great, I think the thrust is not present original research.
%	\item A conclusion could contain something like tasks which are ideal for DNN etc. and tasks which are out of its scope."
%	% This is an excellent point :-)
%\end{enumerate}
%	% From Dafydd's mail Aug 21, 2016 at 12:21 PM 
%\begin{verbatim} 
%\begin{enumerate}
%	\item 
%\end{enumerate}
%
%What is ML?
%What is rule-based learning (i.e. generalisation, 
% subsumption, automaton learning)?
% What is statistical learning?
% What is "training"?
% What is the difference between supervised and unsupervised learning?
% What is the difference between classification and sequence learning?
% What are NNs/ANNs?
%What are DNNs and what is the essential literature on them?
%How do they relate to ML?
%
%\begin{enumerate}
%
%\item How are DNNs developed and evaluated in practice?
%\item Where can working demos of DNNs be found?
%\item Who does ML and who does DNNs?
%\item What are the results of ML and DNNs in comparable tasks?
%\item How do ML and DNN relate to current topics such as big data, 
%surveillance, chess-playing, etc.?
%\item What kind of insights can ML and DNN provide for linguists?
%\item How do DNNs relate to first language acquisition and foreign 
%language learning? 
%\end{enumerate}
%\end{verbatim}
\section{Introduction}
Deep learning as a broad framework of methods has taken speech and natural language processing system-building by a storm. The deluge of work that has happened in the last 25 or so years has shown remarkable promise, and growth. Therefore we don't need to underscore the need to present here the progress that has been made since the late 80's. One of the primary goals of our paper, therefore, is to highlight some of the most significant approaches and the findings therein. While our approach mostly will be chronological, we will also focus on how deep neural networks (DNN) have fundamentally revolutionized our approach to speech processing systems in general, and Automatic Speech Recognition (ASR) and Text-to-Speech Synthesis (TTS) in particular. In addition, we want to focus on a very specific aspect within the use of DNNs in speech processing, namely the integration of linguistic knowledge in achieving some of the remarkable successes in the core tasks of speech processing. At the outset, we would like to outline that the goals of this paper are not to introduce the concepts of machine learning, but to specifically treat a class of learning algorithms that variously appear in the literature under the cover term deep learning. Essentially, all deep learning systems and architectures are a specific form of artificial neural networks. 

In a series of seminal papers, \cite{bengio1989_acm,bengio1989} outline the use of multilayered neural networks in ASR and speaker recognition, respectively. 

\section{The architecture of Deep Neural Netowrks}
Typically, DNNs refer to feedforward multi-layered artificial neural networks (ANN) with more than one layer of hidden units with a logistic function to traverse between the hidden layers and the output. Here we rely on \cite{hinton2012} to outline the general architecture of DNNs. We will illustrate the functioning of the algorithms and the processes with an acoustic modeling task as discussed in \cite{hinton2012}. 
\begin{center}

\begin{equation}
y_{j}=logistic(x_{j})=\frac{1}{1+e^{-x_{j}}}, x_{j} = b_{j} + \sum_{i}y_{i}w_{ij},

\end{equation}

\begin{equation}
p_{j}=\frac{exp(x_{j})}{\sum_^{k}exp(x_{k})}
\end{equation}

\begin{equation}
C=-{\sum_^{j}}d_{j} log p_{j}
\end{equation}

\begin{equation}
\Delta w_{ij}(t)=\alpha\Delta w_{ij}(t-1) - \epsilon \frac{\delta C}{\delta w_{ij}(t)}
\end{equation}

\end{center}

\bibliographystyle{natbib}
\bibliography{llc_biblio}
\end{document}